{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac56fdd",
   "metadata": {},
   "source": [
    "### LangChain chatbot with HeatWave GenAI\n",
    "With the launch of [HeatWave GenAI](https://www.oracle.com/heatwave/genai/), the platform's capabilities now extend to unstructured data, empowering enterprises to harness the potential of Generative AI. HeatWave GenAI provides a unified environment for Generative AI, integrating all stages of the AI pipeline within the database. It features in-database LLMs, in-database embedding generation, and works seamlessly with other in-database functions such as machine learning, analytics, and Lakehouse. Additionally, HeatWave GenAI allows users the flexibility to leverage external LLM services like [OCI Generative AI](https://www.oracle.com/artificial-intelligence/generative-ai/generative-ai-service/).\n",
    "\n",
    "[LangChain](https://github.com/langchain-ai/langchain) is a framework designed to simplify the development of applications powered by large language models (LLMs). It provides a set of tools and abstractions to build complex workflows, allowing developers to integrate LLMs with external data sources, APIs, and systems.\n",
    "\n",
    "HeatWave GenAI can be easily integrated with LangChain applications by specifying a [custom LLM](https://python.langchain.com/docs/how_to/custom_llm/) wrapper class and then using it like any other LLM within LangChain. We use this custom MyLLM class to define a simple LangChain chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f85447-8468-412c-bcab-1db73ac690c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ssaagraw/Documents/GitHub/heatwave-ml/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pydantic import Field\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from langchain_core.language_models import LLM\n",
    "from langchain_core.messages import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08953a9",
   "metadata": {},
   "source": [
    "### Connect to the HeatWave instance\n",
    "We create a connection to an active [HeatWave](https://www.oracle.com/mysql/) instance using the [MySQL Connector/Python](https://dev.mysql.com/doc/connector-python/en/). We also define an API to execute a SQL query using a cursor, and the result is returned as a Pandas DataFrame. Modify the below variables to point to your HeatWave instance. On AWS, set USE_BASTION to False. On OCI, please create a tunnel on your machine using the below command by substituting the variable with their respective values.\n",
    "\n",
    "ssh -o ServerAliveInterval=60 -i BASTION_PKEY -L LOCAL_PORT:DBSYSTEM_IP:DBSYSTEM_PORT BASTION_USER@BASTION_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f71304",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASTION_IP = \"ip_address\"\n",
    "BASTION_USER = \"opc\"\n",
    "BASTION_PKEY = \"private_key_file\"\n",
    "DBSYSTEM_IP = \"ip_address\"\n",
    "DBSYSTEM_PORT = 3306\n",
    "DBSYSTEM_USER = \"username\"\n",
    "DBSYSTEM_PASSWORD = \"password\"\n",
    "DBSYSTEM_SCHEMA = \"ml_benchmark\"\n",
    "LOCAL_PORT = 31231\n",
    "USE_BASTION = True\n",
    "\n",
    "if USE_BASTION is True:\n",
    "    DBSYSTEM_IP = \"127.0.0.1\"\n",
    "else:\n",
    "    LOCAL_PORT = DBSYSTEM_PORT\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host=DBSYSTEM_IP,\n",
    "    port=LOCAL_PORT,\n",
    "    user=DBSYSTEM_USER,\n",
    "    password=DBSYSTEM_PASSWORD,\n",
    "    database=DBSYSTEM_SCHEMA,\n",
    "    allow_local_infile=True,\n",
    "    use_pure=True,\n",
    "    autocommit=True,\n",
    ")\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "\n",
    "# Helper function to execute SQL queries and return the results as a Pandas DataFrame\n",
    "def execute_sql(sql: str) -> pd.DataFrame:\n",
    "    mycursor.execute(sql)\n",
    "    return pd.DataFrame(mycursor.fetchall(), columns=mycursor.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c4328-e9d9-4493-974a-dff4f5a8597e",
   "metadata": {},
   "source": [
    "### Define a custom class for MySQL LLMs on top of LangChain's LLM interface\n",
    "The llm object can now be used as a part of any LangChain invocation. Here is an example of using the above LLM as a Chatbot as described in this LangChain tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d9e269-47a5-4de6-9c59-bce18a275f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLLM(LLM):\n",
    "    \"\"\"\n",
    "    Custom class for HeatWave LLMs\n",
    "    \"\"\"\n",
    "\n",
    "    # HeatWave GenAI LLM to use. Can be an in-HeatWave LLM or OCI Generative AI LLM.\n",
    "    # For the list of supported LLMs, refer to\n",
    "    # https://dev.mysql.com/doc/heatwave/en/mys-hw-genai-supported-models.html\n",
    "    model_id: str = \"llama3.2-3b-instruct-v1\"\n",
    "\n",
    "    # Helper functionper function to execute SQL queries and return the results as a Pandas DataFrame\n",
    "    def _execute_sql(self, sql: str) -> pd.DataFrame:\n",
    "        mycursor.execute(sql)\n",
    "        return pd.DataFrame(mycursor.fetchall(), columns=mycursor.column_names)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Run the LLM on the given input.\n",
    "\n",
    "        Args:\n",
    "            prompt: The prompt to generate from.\n",
    "            stop: Not supported.\n",
    "\n",
    "            **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
    "                to the model provider API call.\n",
    "\n",
    "        Returns:\n",
    "            The model output as a string. Actual completions SHOULD NOT include the prompt.\n",
    "        \"\"\"\n",
    "        if stop is not None:\n",
    "            raise NotImplementedError(\"stop kwargs are not permitted.\")\n",
    "        output = self._execute_sql(\n",
    "            f\"\"\"SELECT sys.ML_GENERATE(\"{prompt}\", JSON_OBJECT(\"task\", \"generation\", \"model_id\", \"{self.model_id}\"));\"\"\"\n",
    "        )\n",
    "        return json.loads(output.iat[0, 0])[\"text\"]\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956beb73-44d5-4fa0-94b6-07cbbf0dab07",
   "metadata": {},
   "source": [
    "### Create a chatbot using MyLLM\n",
    "We first create an llm object by instantiating the MyLLM class. The llm object can now be used as a part of any LangChain invocation. Here is an example of using the above LLM as a Chatbot as described in this [LangChain tutorial](https://python.langchain.com/docs/tutorials/chatbot/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd966ca-e033-4e43-b4af-41c1d3d4f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have any information about your name, Bob. You just told me that you're Bob. Would you like to tell me a bit more about yourself or is there something specific I can help you with?\n"
     ]
    }
   ],
   "source": [
    "llm = MyLLM()\n",
    "print(\n",
    "    llm.invoke(\n",
    "        [\n",
    "            HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "            AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "            HumanMessage(content=\"What's my name?\"),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3f2ce0",
   "metadata": {},
   "source": [
    "The above response was generated using the llama3.2-3b-instruct-v1 LLM (default in the above custom class) which is running inside HeatWave on CPUs at no additional cost to you. \n",
    "\n",
    "As you can see, using HeatWave GenAI with LangChain is simple, and the above custom class should serve most usecases. You can of course extend the above class to add more supported options like temperature, max_tokens or any other supported parameter.\n",
    "\n",
    "We invite you to try [HeatWave AutoML and GenAI](https://www.oracle.com/heatwave/free/). If youâ€™re new to Oracle Cloud Infrastructure, try Oracle Cloud Free Trial, a free 30-day trial with US$300 in credits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
