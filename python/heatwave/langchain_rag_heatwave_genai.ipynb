{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd489898",
   "metadata": {},
   "source": [
    "## LangChain Vector store and RAG with HeatWave GenAI\n",
    "\n",
    "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) workflow powered by [HeatWave GenAI](https://www.oracle.com/heatwave/genai/) on MySQL HeatWave. You will ingest content from a public web page, chunk and index it into the HeatWave Vector Store, and then use a HeatWave-hosted LLM to answer questions grounded in the retrieved context, all within the familiar [LangChain](https://github.com/langchain-ai/langchain) framework. This notebook follows the LangChain guide to [creating vector stores and using them for RAG](https://python.langchain.com/v0.2/docs/tutorials/rag/). \n",
    "\n",
    "### What you’ll build\n",
    "\n",
    "- Data ingestion: Load a LangChain blog post via WebBaseLoader.\n",
    "- Chunking: Split the text into overlapping chunks for better recall.\n",
    "- Indexing: Create embeddings with MyEmbeddings and store them in MyVectorStore on HeatWave.\n",
    "- Retrieval: Convert the vector store into a retriever to fetch relevant chunks at query time.\n",
    "- Generation: Use a reusable RAG prompt from the LangChain Hub and MyLLM to produce grounded answers.\n",
    "\n",
    "### Workflow overview\n",
    "\n",
    "- Load the source page and produce Document objects.\n",
    "- Split documents into chunks (size 1000, overlap 200).\n",
    "- Embed and index chunks in HeatWave Vector Store.\n",
    "- Retrieve top matches for a user question.\n",
    "- Construct the final prompt (context + question) and generate an answer with HeatWave GenAI.\n",
    "\n",
    "All vector operations and inference calls are routed through HeatWave GenAI components (MyEmbeddings, MyVectorStore, MyLLM) for low-latency, in-database AI.\n",
    "\n",
    "**This requires mysql-connector-python>=9.5.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08953a9",
   "metadata": {},
   "source": [
    "### Connect to the HeatWave instance\n",
    "We create a connection to an active HeatWave instance using the MySQL Connector/Python. We also define an API to execute a SQL query using a cursor, and the result is returned as a Pandas DataFrame. Modify the below variables to point to your HeatWave instance. On AWS, set USE_BASTION to False. On OCI, please create a tunnel on your machine using the below command by substituting the variable with their respective values.\n",
    "\n",
    "ssh -o ServerAliveInterval=60 -i BASTION_PKEY -L LOCAL_PORT:DBSYSTEM_IP:DBSYSTEM_PORT BASTION_USER@BASTION_IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f71304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "BASTION_IP = \"ip_address\"\n",
    "BASTION_USER = \"opc\"\n",
    "BASTION_PKEY = \"private_key_file\"\n",
    "DBSYSTEM_IP = \"127.0.0.1\"\n",
    "DBSYSTEM_PORT = 3306\n",
    "DBSYSTEM_USER = \"root\"\n",
    "DBSYSTEM_PASSWORD = \"\"\n",
    "DBSYSTEM_SCHEMA = \"ml_benchmark\"\n",
    "LOCAL_PORT = 31231\n",
    "USE_BASTION = False\n",
    "\n",
    "if USE_BASTION is True:\n",
    "    DBSYSTEM_IP = \"127.0.0.1\"\n",
    "else:\n",
    "    LOCAL_PORT = DBSYSTEM_PORT\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "    host=DBSYSTEM_IP,\n",
    "    port=LOCAL_PORT,\n",
    "    user=DBSYSTEM_USER,\n",
    "    password=DBSYSTEM_PASSWORD,\n",
    "    database=DBSYSTEM_SCHEMA,\n",
    "    allow_local_infile=True,\n",
    "    use_pure=True,\n",
    "    autocommit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956beb73-44d5-4fa0-94b6-07cbbf0dab07",
   "metadata": {},
   "source": [
    "### Retrieve and parse a LangChain blog using the WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a827aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from mysql.ai.genai import MyEmbeddings, MyVectorStore, MyLLM\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(web_paths=(\"https://blog.langchain.com/deep-agents/\",))\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef96d813",
   "metadata": {},
   "source": [
    "### Split the retrieved page into smaller chunks to create meaningful embeddings.\n",
    "This step partitions the source text into manageable segments while preserving continuity between them. By aiming for a target size and introducing a small overlap, it maintains context that might otherwise be lost at boundaries, improving downstream representation and recall. The recursive strategy prefers higher-level breakpoints (e.g., paragraphs or sentences) before falling back to finer splits, producing chunks that are both structurally coherent and well-suited for embedding, indexing, and retrieval in knowledge-grounded generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e037d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b83a05",
   "metadata": {},
   "source": [
    "### Embed and store the chunks into the MySQL HeatWave Vector store\n",
    "Initialize a vector-backed database powered by HeatWave GenAI and populate it with the prepared text segments. An embedding model is attached to the store so that each chunk is transformed into a dense vector representation at ingestion time. By adding the documents, the system builds an efficient similarity-search surface over the corpus, enabling later retrieval of semantically related passages that can ground downstream generation and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c918024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = MyVectorStore(mydb, MyEmbeddings(mydb))\n",
    "_ = vectorstore.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773635c3",
   "metadata": {},
   "source": [
    "### Create a LangChain retriever and RAG prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb0838f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/py_3.11/lib64/python3.11/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810cda0e",
   "metadata": {},
   "source": [
    "### Create a RAG chain \n",
    "Assemble the retrieval-augmented generation flow. A small formatter condenses retrieved documents into a clean context block by joining their text, while the question is passed through unchanged. The pipeline then fuses context and question into a reusable prompt, invokes the HeatWave GenAI model to generate an answer grounded in that context, and finally normalizes the output to plain text. The result is a streamlined orchestration that couples retrieval with generation, turning relevant snippets into coherent, answer-ready responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870e6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "llm = MyLLM(mydb)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07130d2",
   "metadata": {},
   "source": [
    "### Invoke the RAG chain with a user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6b3d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep agents are advanced AI models that can plan over longer time horizons and execute complex tasks, allowing them to dive deeper into topics and achieve more sophisticated results. They use the same core algorithm as LLMs but with additional features such as detailed system prompts, planning tools, sub-agents, and a virtual file system. These agents are capable of executing more complex tasks and can be customized with custom prompts, tools, and sub-agents to suit specific needs.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What are deep agents?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84be7de",
   "metadata": {},
   "source": [
    "We invite you to try [HeatWave AutoML and GenAI](https://www.oracle.com/heatwave/free/). If you’re new to Oracle Cloud Infrastructure, try Oracle Cloud Free Trial, a free 30-day trial with US$300 in credits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
